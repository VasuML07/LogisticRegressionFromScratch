Logistic Regression from ScratchThis repository contains a pure Python implementation of the Logistic Regression algorithm, built using the NumPy library. This project demonstrates the fundamental mathematics and logic behind binary classification models without relying on high-level machine learning frameworks like Scikit-Learn or TensorFlow.OverviewLogistic Regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature (e.g., 0 or 1, True or False). This implementation covers:Parameter initialization (Weights and Bias)The Sigmoid activation functionForward propagationGradient Descent optimizationBinary Cross-Entropy loss calculationPrediction logicMathematical BackgroundThis implementation relies on the following mathematical concepts:1. Activation Function (Sigmoid)To map predictions to probabilities between 0 and 1, the sigmoid function is used:$$\sigma(z) = \frac{1}{1 + e^{-z}}$$2. Linear ModelThe input features are combined linearly with weights and bias:$$z = w \cdot X + b$$3. Cost FunctionThe model minimizes the Binary Cross-Entropy loss:$$J(w,b) = -\frac{1}{m} \sum [y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$4. Gradient DescentParameters are updated iteratively using the gradients of the cost function:$$w = w - \alpha \cdot dw$$$$b = b - \alpha \cdot db$$PrerequisitesTo run this code, you need Python installed along with the NumPy library.Bashpip install numpy
UsageYou can import the LogisticRegression class into your project or run the script directly.ExamplePythonimport numpy as np
from logistic_regression import LogisticRegression

# 1. Prepare data
X_train = np.array([[10.0, 10.0], [11.0, 15.0], [19.0, 15.0], [18.0, 20.0]])
y_train = np.array([0, 0, 1, 1])

# 2. Initialize the model
# learning_rate: step size for gradient descent
# iter: number of training iterations
model = LogisticRegression(learning_rate=0.01, iter=500)

# 3. Train the model
model.fit(X_train, y_train)

# 4. Make predictions
X_test = np.array([[17.5, 22.0]])
prediction = model.predict(X_test)

print(f"Predicted Class: {prediction[0]}")
Class DocumentationLogisticRegressionParameters:learning_rate (float): The step size used for updating weights during training. Default is 0.01.iter (int): The number of iterations (epochs) to run gradient descent. Default is 100.Methods:fit(X, y)Trains the logistic regression model on the given dataset.X: Training vector (features). Shape: (m_samples, n_features)y: Target vector (labels). Shape: (m_samples,)predict(X)Predicts class labels for samples in X. It applies the threshold of 0.5 to the probabilities generated by the sigmoid function.Returns: An array of binary class labels (0 or 1).sigmoid(z)A helper method that computes the sigmoid of input z.LicenseThis project is open-source and available for educational purposes.
