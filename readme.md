Logistic Regression From Scratch (NumPy)

This repository contains a from-scratch implementation of Logistic Regression using Python and NumPy only. The goal is to demonstrate how binary classification works at a mathematical and algorithmic level, without relying on machine learning libraries such as Scikit-learn or TensorFlow.

The implementation explicitly shows how probabilities are computed, how loss is calculated, and how parameters are optimized using gradient descent.

What the Model Does

Logistic Regression predicts binary outcomes (0 or 1) by estimating the probability that an input belongs to a particular class.

The model follows this flow:

Input Features (X)
â†’ Linear Combination (z = wÂ·x + b)
â†’ Sigmoid Activation
â†’ Probability (0 to 1)
â†’ Threshold (0.5)
â†’ Final Class Prediction

Core Mathematics
Linear Model
ğ‘§
=
ğ‘¤
1
ğ‘¥
1
+
ğ‘¤
2
ğ‘¥
2
+
â‹¯
+
ğ‘¤
ğ‘›
ğ‘¥
ğ‘›
+
ğ‘
z=w
1
	â€‹

x
1
	â€‹

+w
2
	â€‹

x
2
	â€‹

+â‹¯+w
n
	â€‹

x
n
	â€‹

+b
Sigmoid Activation
ğœ
(
ğ‘§
)
=
1
1
+
ğ‘’
âˆ’
ğ‘§
Ïƒ(z)=
1+e
âˆ’z
1
	â€‹


This function converts any real number into a probability between 0 and 1.

Binary Cross-Entropy Loss
ğ¿
=
âˆ’
1
ğ‘š
âˆ‘
[
ğ‘¦
log
â¡
(
ğ‘¦
^
)
+
(
1
âˆ’
ğ‘¦
)
log
â¡
(
1
âˆ’
ğ‘¦
^
)
]
L=âˆ’
m
1
	â€‹

âˆ‘[ylog(
y
^
	â€‹

)+(1âˆ’y)log(1âˆ’
y
^
	â€‹

)]

The loss measures how far the predicted probabilities are from the true labels.

Gradient Descent Updates
ğ‘¤
:
=
ğ‘¤
âˆ’
ğ›¼
â‹…
1
ğ‘š
ğ‘‹
ğ‘‡
(
ğ‘¦
^
âˆ’
ğ‘¦
)
w:=wâˆ’Î±â‹…
m
1
	â€‹

X
T
(
y
^
	â€‹

âˆ’y)
ğ‘
:
=
ğ‘
âˆ’
ğ›¼
â‹…
1
ğ‘š
âˆ‘
(
ğ‘¦
^
âˆ’
ğ‘¦
)
b:=bâˆ’Î±â‹…
m
1
	â€‹

âˆ‘(
y
^
	â€‹

âˆ’y)

This process is repeated for multiple iterations until the model converges.

Implementation Details

Weights and bias are initialized to zero

Predictions are computed using a linear model + sigmoid

Gradients are calculated manually

Parameters are updated using gradient descent

Final predictions use a threshold of 0.5

Everything is implemented explicitly to mirror the math.

Example
Input:  [17.5, 22.0]
Output: Class 1


Internally, the model predicts a probability and then converts it into a class label.

Requirements

Python 3.x

NumPy

No other dependencies.

Why This Repo Exists

This project is meant to build real intuition for machine learning by stripping away abstractions. If you understand this implementation, you understand the foundation behind neural networks, classifiers, and modern deep learning systems.
