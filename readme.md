Logistic Regression From Scratch (NumPy-Based)
Project Overview

This repository contains a from-scratch implementation of Logistic Regression using Python and NumPy only.
No Scikit-learn models. No TensorFlow. No shortcuts.

The purpose of this project is to explain binary classification at the mathematical level, showing how probabilities are computed, how errors are minimized, and how parameters are learned through gradient descent.

This implementation focuses on clarity over abstraction.

Why This Project Matters

Logistic Regression is often treated as a â€œsimpleâ€ algorithm.
In reality, it introduces several foundational ideas that appear everywhere in modern machine learning:

Probability modeling

Non-linear activation functions

Loss-based optimization

Gradient descent

Decision boundaries

Understanding this implementation means you understand the backbone of neural networks and classifiers.

What Logistic Regression Does

Logistic Regression predicts binary outcomes (0 or 1) by estimating the probability that an input belongs to a particular class.

Examples:

Spam vs Not Spam

Fraud vs Legitimate

Malignant vs Benign

Pass vs Fail

Instead of predicting raw values, the model predicts probabilities and then converts them into class labels.

High-Level Model Flow
Input Features (X)
        â†“
Linear Combination (z = wÂ·x + b)
        â†“
Sigmoid Activation
        â†“
Probability (0 â†’ 1)
        â†“
Threshold (0.5)
        â†“
Final Class (0 or 1)

Mathematical Foundations
1. Linear Model

The model starts by computing a weighted sum of the input features:

ğ‘§
=
ğ‘¤
1
ğ‘¥
1
+
ğ‘¤
2
ğ‘¥
2
+
â‹¯
+
ğ‘¤
ğ‘›
ğ‘¥
ğ‘›
+
ğ‘
z=w
1
	â€‹

x
1
	â€‹

+w
2
	â€‹

x
2
	â€‹

+â‹¯+w
n
	â€‹

x
n
	â€‹

+b

Where:

ğ‘¤
w are the learned weights

ğ‘
b is the bias

ğ‘¥
x is the input feature vector

This step alone is not enough for classification.

2. Sigmoid Activation Function

To convert the linear output into a probability, the model applies the Sigmoid function:

ğœ
(
ğ‘§
)
=
1
1
+
ğ‘’
âˆ’
ğ‘§
Ïƒ(z)=
1+e
âˆ’z
1
	â€‹


Key properties:

Output range: (0, 1)

Smooth and differentiable

Ideal for probability estimation

Sigmoid Curve (Conceptual)

1.0 â”¤            ______
    â”‚          /
0.5 â”¤--------â€¢--------
    â”‚       /
0.0 â”¼______/__________
           z

3. Binary Cross-Entropy Loss

To measure how wrong the predictions are, the model uses Binary Cross-Entropy Loss:

ğ¿
=
âˆ’
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
[
ğ‘¦
ğ‘–
log
â¡
(
ğ‘¦
^
ğ‘–
)
+
(
1
âˆ’
ğ‘¦
ğ‘–
)
log
â¡
(
1
âˆ’
ğ‘¦
^
ğ‘–
)
]
L=âˆ’
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

[y
i
	â€‹

log(
y
^
	â€‹

i
	â€‹

)+(1âˆ’y
i
	â€‹

)log(1âˆ’
y
^
	â€‹

i
	â€‹

)]

Why this loss?

Strongly penalizes confident wrong predictions

Works naturally with probabilities

Provides smooth gradients for optimization

Lower loss means better predictions.

4. Gradient Descent Optimization

The model learns by updating parameters in the direction that reduces loss.

Gradients

Weight gradient:

âˆ‚
ğ¿
âˆ‚
ğ‘¤
=
1
ğ‘š
ğ‘‹
ğ‘‡
(
ğ‘¦
^
âˆ’
ğ‘¦
)
âˆ‚w
âˆ‚L
	â€‹

=
m
1
	â€‹

X
T
(
y
^
	â€‹

âˆ’y)

Bias gradient:

âˆ‚
ğ¿
âˆ‚
ğ‘
=
1
ğ‘š
âˆ‘
(
ğ‘¦
^
âˆ’
ğ‘¦
)
âˆ‚b
âˆ‚L
	â€‹

=
m
1
	â€‹

âˆ‘(
y
^
	â€‹

âˆ’y)
5. Parameter Update Rule
ğ‘¤
:
=
ğ‘¤
âˆ’
ğ›¼
â‹…
ğ‘‘
ğ‘¤
w:=wâˆ’Î±â‹…dw
ğ‘
:
=
ğ‘
âˆ’
ğ›¼
â‹…
ğ‘‘
ğ‘
b:=bâˆ’Î±â‹…db

Where:

ğ›¼
Î± is the learning rate

This process is repeated over multiple iterations until convergence.

Training Workflow

Initialize weights and bias to zero

Compute linear predictions

Apply sigmoid activation

Calculate loss

Compute gradients

Update parameters

Repeat for fixed iterations

This loop is the engine of learning.

Code Structure Overview

LogisticRegression class

sigmoid(z) â€“ activation function

fit(X, y) â€“ training via gradient descent

predict(X) â€“ class prediction using threshold

The implementation mirrors the math line by line, making it ideal for learning and debugging.

Example Usage
X = [[10,10], [11,15], [12,12], [19,15], [18,20]]
y = [0, 0, 0, 1, 1]

model = LogisticRegression(learning_rate=0.01, iter=500)
model.fit(X, y)

prediction = model.predict([[17.5, 22.0]])

Output Interpretation

Model outputs probabilities internally

A threshold of 0.5 is applied:

Probability â‰¥ 0.5 â†’ Class 1

Probability < 0.5 â†’ Class 0

This creates a linear decision boundary in feature space.

Learning Outcomes

This project builds strong intuition for:

Probability-based classification

Optimization using gradients

Loss-driven learning

Foundations of neural networks

Why sigmoid + cross-entropy works so well

Everything here scales directly to deep learning.

Requirements

Python 3.x

NumPy

Nothing else.

Future Improvements

Add loss tracking and visualization

Extend to multi-class classification (Softmax)

Implement regularization

Compare with Scikit-learn output
