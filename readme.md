Logistic Regression from Scratch
This repository contains a pure Python implementation of the Logistic Regression algorithm, built using the NumPy library. This project demonstrates the fundamental logic behind binary classification models without relying on high-level machine learning frameworks like Scikit-Learn or TensorFlow.

Overview
Logistic Regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature (e.g., 0 or 1, True or False). This implementation covers:

Parameter initialization (Weights and Bias)

The Sigmoid activation function

Forward propagation

Gradient Descent optimization

Binary Cross-Entropy loss calculation

Prediction logic

Technical Concepts
This implementation relies on the following core concepts:

1. Activation Function (Sigmoid) The model uses the Sigmoid function to map predictions to probabilities. This function takes any real-valued number and compresses it into a value between 0 and 1. This is essential for binary classification to determine the probability of an input belonging to a specific class.

2. Linear Model The algorithm calculates a linear relationship between inputs and outputs. It computes a weighted sum of the input features and adds a bias term. This raw output is then passed to the activation function.

3. Cost Function To evaluate the performance of the model, the code calculates the Binary Cross-Entropy loss. This metric quantifies the difference between the predicted probabilities and the actual class labels. A lower cost indicates a better-performing model.

4. Gradient Descent The model optimizes its parameters using Gradient Descent. This is an iterative process where the algorithm calculates the gradient (the direction of steepest increase) of the cost function. It then updates the weights and bias in the opposite direction to minimize the error.
Class Documentation
LogisticRegression
Parameters:

learning_rate (float): The step size used for updating weights during training. A smaller value ensures convergence but may be slower, while a larger value may overshoot the minimum. Default is 0.01.

iter (int): The number of iterations (epochs) to run the gradient descent algorithm. Default is 100.

Methods:

fit(X, y) Trains the logistic regression model on the given dataset.

X: Training vector containing the features.

y: Target vector containing the labels.

predict(X) Predicts class labels for new samples. It applies a threshold of 0.5 to the probabilities generated by the sigmoid function to determine the final class (0 or 1).

Returns: An array of binary class labels.

sigmoid(z) A helper method that computes the sigmoid activation for a given input.
